{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on tweets\n",
    "\n",
    "*Objective* To perform sentiment analysis on tweets.\n",
    "Training data used is the [sentiment140 training][training_set] set, which contains 1.6 Million tweets.\n",
    "The tweets are classified into positive, and negative.\n",
    "\n",
    "For this experiment I will be using a LSTM model from the Keras library with Tensorflow as the backend. \n",
    "\n",
    "[training_set]: http://help.sentiment140.com/for-students/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "DATA_DIR = BASE_DIR + 'data/'\n",
    "GLOVE_DIR = BASE_DIR + 'glove_dir/'\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR,\"training.1600000.processed.noemoticon.csv\"), names=(['polarity', 'tweet_id', 'date', 'query', 'user', 'text']), encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode Polarity to 0 an 1, instead of the 0(negative) and 4(positive), \n",
    "so that keras is able to recognise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['isNegative'] = df['polarity'].map({0: 1, 2: 0, 4: 0})\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the GloVe embeddings \n",
    "I will be using the 100 dimensional [GloVe][glove] embeddings, of 400k words.\n",
    "\n",
    "[glove]: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../glove_dir/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {0} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will proceed to splitting our data into train set and test set.\n",
    "Which will be followed by tokenizing the tweet texts and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 unique tokens.\n",
      "x_train shape: (1280000, 80)\n",
      "x_test shape: (320000, 80)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "max_words = 1000\n",
    "batch_size = 32\n",
    "maxlen = 80 # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['text'], df[['isNegative', 'isNeutral', 'isPositive']], test_size=0.2, random_state=12)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['isNegative'], test_size=0.2, random_state=12)\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True, filters='0123456789.#!?:()[]', char_level=True)\n",
    "tokenizer.fit_on_texts(np.array(df['text'].fillna('')))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found {0} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(np.array(X_train)), maxlen=maxlen) \n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(np.array(X_test)), maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the embedding matrix which will be used as the weights in our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        \n",
    "        embedding_matrix[i-1] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train our model\n",
    "\n",
    "We will start creating our model which will be using a convolutional layer followed by a LSTM layer.\n",
    "\n",
    "*Note* Embedding layer should have trainable set to False, so that weights we computed earlier does not get overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024000 samples, validate on 256000 samples\n",
      "Epoch 1/20\n",
      "1024000/1024000 [==============================] - 2776s - loss: 0.5596 - acc: 0.7053 - val_loss: 0.4920 - val_acc: 0.7594\n",
      "Epoch 2/20\n",
      "1024000/1024000 [==============================] - 2766s - loss: 0.5016 - acc: 0.7533 - val_loss: 0.4640 - val_acc: 0.7781\n",
      "Epoch 3/20\n",
      "1024000/1024000 [==============================] - 2851s - loss: 0.4887 - acc: 0.7624 - val_loss: 0.4554 - val_acc: 0.7840\n",
      "Epoch 4/20\n",
      "1024000/1024000 [==============================] - 2850s - loss: 0.4828 - acc: 0.7664 - val_loss: 0.4535 - val_acc: 0.7865\n",
      "Epoch 5/20\n",
      "1024000/1024000 [==============================] - 2863s - loss: 0.4804 - acc: 0.7681 - val_loss: 0.4518 - val_acc: 0.7872\n",
      "Epoch 6/20\n",
      "1024000/1024000 [==============================] - 2856s - loss: 0.4782 - acc: 0.7700 - val_loss: 0.4486 - val_acc: 0.7889\n",
      "Epoch 7/20\n",
      "1024000/1024000 [==============================] - 2864s - loss: 0.4779 - acc: 0.7702 - val_loss: 0.4468 - val_acc: 0.7896\n",
      "Epoch 8/20\n",
      "1024000/1024000 [==============================] - 2862s - loss: 0.4773 - acc: 0.7712 - val_loss: 0.4484 - val_acc: 0.7881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8381cf0f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input = Input(shape=(80,), name='tweet_input')\n",
    "embedding_layer = Embedding(num_words,EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlen,trainable=False)(tweet_input)\n",
    "x = Dropout(0.2)(embedding_layer)\n",
    "convolution = Conv1D(64, 64, padding='valid', activation='relu', strides=1)(x)\n",
    "pooling = MaxPooling1D(pool_size=4)(convolution)\n",
    "#lstm = LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "lstm = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "lstm_out = Dense(1, activation='sigmoid', name='lstm_out')(lstm)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model = Model(inputs=[tweet_input], outputs=[lstm_out])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "#print (model.summary())\n",
    "metrics=['accuracy']\n",
    "mode = 'auto'\n",
    "monitor = 'val_acc'\n",
    "patience = 0\n",
    "cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n",
    "model.fit(X_train, np.array(y_train), validation_split=VALIDATION_SPLIT, epochs=20, callbacks=cbks, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each epoch took around 50 min to complete. For refernce I ran this on my core i7 laptop.\n",
    "This was however not run on a tensorflow configured for cuda.\n",
    "\n",
    "The model finally shows an accuracy of 0.7881 in the validation set.\n",
    "Now let's see how this model performs on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319968/320000 [============================>.] - ETA: 0s\n",
      " Test loss: 0.44714794987887146, acc: 0.789975\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(np.array(X_test), np.array(y_test), batch_size=batch_size, verbose=1)\n",
    "print(\"\\n Test {0}: {1}, {2}: {3}\".format(model.metrics_names[0], scores[0], model.metrics_names[1], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our model on some live data and see how the model fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pred_test = [\"I know people love them, but I find C's prefix and postfix increment operators a usability nightmare.\",\n",
    "              \"I understand that people don't want to read research papers, so it's a non-issue. But it's just so blatant and shameless.\",\n",
    "              \"Awesome work from @OpenAI: Dota 2 bot, a neural net trained with self-play beats the world's top players at 1v1. Learns fun strategies!\",\n",
    "              \"Finally watched Silicon Valley season 4 this week... I laughed so hard at this scene:\",\n",
    "              \"Oh, I know what would make verbose arithmetic code easier to read: adding in mutation as a concept to basic arithmetic.\",\n",
    "              \"\"\"The way in which you are a \"glas half full\" person where I'm a \"glass half empty and why is there no ice\" person are amazing ;)\"\"\"]\n",
    "\n",
    "X_pred_test = pad_sequences(tokenizer.texts_to_sequences(np.array(X_pred_test)), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62591708],\n",
       "       [ 0.69186771],\n",
       "       [ 0.37482822],\n",
       "       [ 0.71552372],\n",
       "       [ 0.49691495],\n",
       "       [ 0.51810449]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, this does not look really good. All output values are very close 0.5 , so they are not very confident.\n",
    "But it's surprising that the classifier was able to identify the last two tweets were negative.\n",
    "\n",
    "Finally let's save this model for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('tweet_sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "1. It would be intresting to see the effect of using a higher dimensional embedding matrix.\n",
    "2. Since the tweets often contain "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
